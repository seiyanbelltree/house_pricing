{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantitative = [f for f in train.columns if train.dtypes[f] != 'object']\n",
    "quantitative.remove('SalePrice')\n",
    "quantitative.remove('Id')\n",
    "qualitative = [f for f in train.columns if train.dtypes[f] == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetOrderEncode:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit_and_transform(self, train, feature):\n",
    "        self.feature = feature\n",
    "        self.ordering = pd.DataFrame()\n",
    "        self.ordering['val'] = train[feature].unique()\n",
    "        self.ordering.index = self.ordering.val\n",
    "        self.ordering['spmean'] = train[[feature, 'SalePrice']].groupby(feature).mean()['SalePrice']\n",
    "        self.ordering = self.ordering.sort_values('spmean')\n",
    "        self.ordering['ordering'] = range(1, self.ordering.shape[0]+1)\n",
    "        self.ordering = self.ordering['ordering'].to_dict()\n",
    "\n",
    "        self.transform(train)\n",
    "\n",
    "    def transform(self, test):\n",
    "        for cat, o in self.ordering.items():\n",
    "            test.loc[test[self.feature] == cat, self.feature+'_E'] = o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MSZoning_E', 'Street_E', 'Alley_E', 'LotShape_E', 'LandContour_E', 'Utilities_E', 'LotConfig_E', 'LandSlope_E', 'Neighborhood_E', 'Condition1_E', 'Condition2_E', 'BldgType_E', 'HouseStyle_E', 'RoofStyle_E', 'RoofMatl_E', 'Exterior1st_E', 'Exterior2nd_E', 'MasVnrType_E', 'ExterQual_E', 'ExterCond_E', 'Foundation_E', 'BsmtQual_E', 'BsmtCond_E', 'BsmtExposure_E', 'BsmtFinType1_E', 'BsmtFinType2_E', 'Heating_E', 'HeatingQC_E', 'CentralAir_E', 'Electrical_E', 'KitchenQual_E', 'Functional_E', 'FireplaceQu_E', 'GarageType_E', 'GarageFinish_E', 'GarageQual_E', 'GarageCond_E', 'PavedDrive_E', 'PoolQC_E', 'Fence_E', 'MiscFeature_E', 'SaleType_E', 'SaleCondition_E']\n"
     ]
    }
   ],
   "source": [
    "qual_encoded = []\n",
    "for q in qualitative:\n",
    "    encoder = TargetOrderEncode()\n",
    "    encoder.fit_and_transform(train, q)\n",
    "    encoder.transform(test)\n",
    "    qual_encoded.append(q+'_E')\n",
    "print(qual_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_features = [\n",
    "    'GrLivArea',\n",
    "    '1stFlrSF',\n",
    "    '2ndFlrSF',\n",
    "    'TotalBsmtSF',\n",
    "    'LotArea',\n",
    "    'LotFrontage',\n",
    "    'KitchenAbvGr',\n",
    "    'GarageArea'\n",
    "]\n",
    "\n",
    "quad_feats = [\n",
    "    'OverallQual',\n",
    "    'YearBuilt',\n",
    "    'YearRemodAdd',\n",
    "    'TotalBsmtSF',\n",
    "    '2ndFlrSF',\n",
    "    'Neighborhood_E',\n",
    "    'RoofMatl_E',\n",
    "    'GrLivArea'\n",
    "]\n",
    "\n",
    "qdr = [f+'2' for f in quad_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(df, feature):\n",
    "    df[feature] = np.log1p(df[feature].values)\n",
    "\n",
    "def quadratic(df, feature):\n",
    "    df[feature+'2'] = df[feature]**2\n",
    "\n",
    "def bool_encode(train):\n",
    "    train['HasBasement'] = train['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    train['HasGarage'] = train['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    train['Has2ndFloor'] = train['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    train['HasMasVnr'] = train['MasVnrArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    train['HasWoodDeck'] = train['WoodDeckSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    train['HasPorch'] = train['OpenPorchSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    train['HasPool'] = train['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    train['IsNew'] = train['YearBuilt'].apply(lambda x: 1 if x > 2000 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in quad_feats:\n",
    "    quadratic(train, f)\n",
    "    quadratic(test, f)\n",
    "\n",
    "for f in log_features:\n",
    "    log_transform(train, f)\n",
    "    log_transform(test, f)\n",
    "\n",
    "bool_encode(train)\n",
    "bool_encode(test)\n",
    "\n",
    "boolean = ['HasBasement', 'HasGarage', 'Has2ndFloor', 'HasMasVnr', 'HasWoodDeck',\n",
    "            'HasPorch', 'HasPool', 'IsNew']\n",
    "\n",
    "features = quantitative + qual_encoded + boolean + qdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(actual, predicted):\n",
    "    return np.sqrt(mean_squared_log_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=216)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11205509234113975\n",
      "0.13071949887035056\n",
      "0.1169484041791938\n",
      "0.12753968125947115\n",
      "0.11915025549134804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksaw/.cache/pypoetry/virtualenvs/house-price-PNXQfjFB-py3.8/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.05281e-16): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "/home/ksaw/.cache/pypoetry/virtualenvs/house-price-PNXQfjFB-py3.8/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.00631e-16): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n"
     ]
    }
   ],
   "source": [
    "X = train[features].fillna(0.).values\n",
    "Y = train['SalePrice'].values\n",
    "\n",
    "models = []\n",
    "Ypred = np.zeros_like(Y)\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_tr = X[train_index]\n",
    "    X_val = X[val_index]\n",
    "    y_tr = Y[train_index]\n",
    "    y_val = Y[val_index]\n",
    "    \n",
    "    ridge = Ridge()\n",
    "    ridge.fit(X_tr, np.log(y_tr))\n",
    "    \n",
    "    y_pred = np.exp(ridge.predict(X_val))\n",
    "    Ypred[val_index] = y_pred\n",
    "    \n",
    "    print(error(y_val, y_pred))\n",
    "    \n",
    "    models.append(ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12147756793269066"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error(Y, Ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = pd.DataFrame()\n",
    "\n",
    "submissions['Ridge'] = Ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[features].fillna(0.).values\n",
    "Y = train['SalePrice'].values\n",
    "X_test = test[features].fillna(0.).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet (nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)   \n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "        self.fc4 = nn.Linear(10, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.2)\n",
    "        self.dropout2 = nn.Dropout2d(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, valid_loader):\n",
    "    net = MLPNet().to(device)\n",
    " \n",
    "    # optimizing\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "    print ('training start ...')\n",
    "    num_epochs = 500  \n",
    "\n",
    "    # initialize list for plot graph after training\n",
    "    train_loss_list, val_loss_list = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # initialize each epoch\n",
    "        train_loss, val_loss = 0, 0\n",
    "\n",
    "        # ======== train_mode ======\n",
    "        net.train()\n",
    "        for i, (x, labels) in enumerate(train_loader):  # ミニバッチ回数実行\n",
    "            x, labels = x.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()  # 勾配リセット\n",
    "            outputs = net(x)  # 順伝播の計算\n",
    "            loss = criterion(torch.flatten(outputs), labels)  # lossの計算\n",
    "            train_loss += loss.item()  # train_loss に結果を蓄積\n",
    "            loss.backward()  # 逆伝播の計算        \n",
    "            optimizer.step()  # 重みの更新\n",
    "            avg_train_loss = np.sqrt(train_loss / len(train_loader.dataset))  # lossの平均を計算\n",
    "\n",
    "        # ======== valid_mode ======\n",
    "        net.eval()\n",
    "        with torch.no_grad():  # 必要のない計算を停止\n",
    "            for x, labels in valid_loader:        \n",
    "                x, labels = x.to(device), labels.to(device)\n",
    "                outputs = net(x)\n",
    "                loss = criterion(torch.flatten(outputs), labels)\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = np.sqrt(val_loss / len(valid_loader.dataset))\n",
    "\n",
    "        # print log\n",
    "        if epoch % 10 == 0:\n",
    "            print ('Epoch [{}/{}], Loss: {loss:.4f}, val_loss: {val_loss:.4f}' \n",
    "                           .format(epoch+1, num_epochs, i+1, loss=avg_train_loss, val_loss=avg_val_loss))\n",
    "\n",
    "        # append list for polt graph after training\n",
    "        train_loss_list.append(avg_train_loss)\n",
    "        val_loss_list.append(avg_val_loss)\n",
    "               \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(X, Y, shuffle=True):\n",
    "    X = torch.tensor(X).float()\n",
    "    Y = torch.tensor(Y).float()\n",
    "    \n",
    "    dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,  # データセットの指定\n",
    "        batch_size=64,  # ミニバッチの指定\n",
    "        shuffle=True,  # シャッフルするかどうかの指定\n",
    "        num_workers=2)  # コアの数\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start ...\n",
      "Epoch [1/500], Loss: 0.7610, val_loss: 0.3881\n",
      "Epoch [11/500], Loss: 0.1439, val_loss: 0.1263\n",
      "Epoch [21/500], Loss: 0.1076, val_loss: 0.0895\n",
      "Epoch [31/500], Loss: 0.1117, val_loss: 0.1062\n",
      "Epoch [41/500], Loss: 0.1146, val_loss: 0.0910\n",
      "Epoch [51/500], Loss: 0.1003, val_loss: 0.0843\n",
      "Epoch [61/500], Loss: 0.1012, val_loss: 0.0766\n",
      "Epoch [71/500], Loss: 0.0959, val_loss: 0.1011\n",
      "Epoch [81/500], Loss: 0.0970, val_loss: 0.0969\n",
      "Epoch [91/500], Loss: 0.1019, val_loss: 0.0939\n",
      "Epoch [101/500], Loss: 0.0940, val_loss: 0.0892\n",
      "Epoch [111/500], Loss: 0.0861, val_loss: 0.0811\n",
      "Epoch [121/500], Loss: 0.0854, val_loss: 0.0896\n",
      "Epoch [131/500], Loss: 0.0867, val_loss: 0.1049\n",
      "Epoch [141/500], Loss: 0.0807, val_loss: 0.0749\n",
      "Epoch [151/500], Loss: 0.1021, val_loss: 0.1029\n",
      "Epoch [161/500], Loss: 0.0737, val_loss: 0.0635\n",
      "Epoch [171/500], Loss: 0.0686, val_loss: 0.0842\n",
      "Epoch [181/500], Loss: 0.0666, val_loss: 0.0603\n",
      "Epoch [191/500], Loss: 0.0661, val_loss: 0.1002\n",
      "Epoch [201/500], Loss: 0.0776, val_loss: 0.0702\n",
      "Epoch [211/500], Loss: 0.0750, val_loss: 0.0564\n",
      "Epoch [221/500], Loss: 0.0574, val_loss: 0.0803\n",
      "Epoch [231/500], Loss: 0.0573, val_loss: 0.0609\n",
      "Epoch [241/500], Loss: 0.0606, val_loss: 0.0732\n",
      "Epoch [251/500], Loss: 0.0627, val_loss: 0.0536\n",
      "Epoch [261/500], Loss: 0.0653, val_loss: 0.0684\n",
      "Epoch [271/500], Loss: 0.0501, val_loss: 0.0441\n",
      "Epoch [281/500], Loss: 0.0554, val_loss: 0.0468\n",
      "Epoch [291/500], Loss: 0.0489, val_loss: 0.0489\n",
      "Epoch [301/500], Loss: 0.0611, val_loss: 0.0585\n",
      "Epoch [311/500], Loss: 0.0525, val_loss: 0.0439\n",
      "Epoch [321/500], Loss: 0.0484, val_loss: 0.0417\n",
      "Epoch [331/500], Loss: 0.0428, val_loss: 0.0513\n",
      "Epoch [341/500], Loss: 0.0416, val_loss: 0.0540\n",
      "Epoch [351/500], Loss: 0.0447, val_loss: 0.0578\n",
      "Epoch [361/500], Loss: 0.0414, val_loss: 0.0424\n",
      "Epoch [371/500], Loss: 0.0499, val_loss: 0.0345\n",
      "Epoch [381/500], Loss: 0.0416, val_loss: 0.0352\n",
      "Epoch [391/500], Loss: 0.0390, val_loss: 0.0370\n",
      "Epoch [401/500], Loss: 0.0338, val_loss: 0.0319\n",
      "Epoch [411/500], Loss: 0.0400, val_loss: 0.0317\n",
      "Epoch [421/500], Loss: 0.0315, val_loss: 0.0325\n",
      "Epoch [431/500], Loss: 0.0290, val_loss: 0.0429\n",
      "Epoch [441/500], Loss: 0.0284, val_loss: 0.0358\n",
      "Epoch [451/500], Loss: 0.0263, val_loss: 0.0255\n",
      "Epoch [461/500], Loss: 0.0292, val_loss: 0.0282\n",
      "Epoch [471/500], Loss: 0.0314, val_loss: 0.0338\n",
      "Epoch [481/500], Loss: 0.0247, val_loss: 0.0239\n",
      "Epoch [491/500], Loss: 0.0296, val_loss: 0.0246\n",
      "0.19598028655950103\n",
      "training start ...\n",
      "Epoch [1/500], Loss: 0.9272, val_loss: 0.4727\n",
      "Epoch [11/500], Loss: 0.1202, val_loss: 0.1494\n",
      "Epoch [21/500], Loss: 0.1154, val_loss: 0.1415\n",
      "Epoch [31/500], Loss: 0.1165, val_loss: 0.1206\n",
      "Epoch [41/500], Loss: 0.1048, val_loss: 0.1160\n",
      "Epoch [51/500], Loss: 0.1146, val_loss: 0.1455\n",
      "Epoch [61/500], Loss: 0.1098, val_loss: 0.2031\n",
      "Epoch [71/500], Loss: 0.1073, val_loss: 0.0927\n",
      "Epoch [81/500], Loss: 0.1031, val_loss: 0.1262\n",
      "Epoch [91/500], Loss: 0.0950, val_loss: 0.1160\n",
      "Epoch [101/500], Loss: 0.0885, val_loss: 0.1284\n",
      "Epoch [111/500], Loss: 0.0915, val_loss: 0.1036\n",
      "Epoch [121/500], Loss: 0.0964, val_loss: 0.1022\n",
      "Epoch [131/500], Loss: 0.0852, val_loss: 0.0988\n",
      "Epoch [141/500], Loss: 0.0893, val_loss: 0.1531\n",
      "Epoch [151/500], Loss: 0.0773, val_loss: 0.0901\n",
      "Epoch [161/500], Loss: 0.0819, val_loss: 0.1351\n",
      "Epoch [171/500], Loss: 0.0742, val_loss: 0.0865\n",
      "Epoch [181/500], Loss: 0.0780, val_loss: 0.0881\n",
      "Epoch [191/500], Loss: 0.0846, val_loss: 0.0937\n",
      "Epoch [201/500], Loss: 0.0767, val_loss: 0.1126\n",
      "Epoch [211/500], Loss: 0.0776, val_loss: 0.0919\n",
      "Epoch [221/500], Loss: 0.0820, val_loss: 0.0803\n",
      "Epoch [231/500], Loss: 0.0687, val_loss: 0.0806\n",
      "Epoch [241/500], Loss: 0.0732, val_loss: 0.0793\n",
      "Epoch [251/500], Loss: 0.0668, val_loss: 0.0763\n",
      "Epoch [261/500], Loss: 0.0779, val_loss: 0.0824\n",
      "Epoch [271/500], Loss: 0.0594, val_loss: 0.1072\n",
      "Epoch [281/500], Loss: 0.0662, val_loss: 0.1072\n",
      "Epoch [291/500], Loss: 0.0594, val_loss: 0.0833\n",
      "Epoch [301/500], Loss: 0.0598, val_loss: 0.0735\n",
      "Epoch [311/500], Loss: 0.0591, val_loss: 0.0656\n",
      "Epoch [321/500], Loss: 0.0538, val_loss: 0.0610\n",
      "Epoch [331/500], Loss: 0.0551, val_loss: 0.0872\n",
      "Epoch [341/500], Loss: 0.0486, val_loss: 0.0481\n",
      "Epoch [351/500], Loss: 0.0441, val_loss: 0.0507\n",
      "Epoch [361/500], Loss: 0.0446, val_loss: 0.0551\n",
      "Epoch [371/500], Loss: 0.0508, val_loss: 0.1163\n",
      "Epoch [381/500], Loss: 0.0402, val_loss: 0.0694\n",
      "Epoch [391/500], Loss: 0.0599, val_loss: 0.0610\n",
      "Epoch [401/500], Loss: 0.0428, val_loss: 0.0511\n",
      "Epoch [411/500], Loss: 0.0369, val_loss: 0.0670\n",
      "Epoch [421/500], Loss: 0.0354, val_loss: 0.0559\n",
      "Epoch [431/500], Loss: 0.0405, val_loss: 0.0489\n",
      "Epoch [441/500], Loss: 0.0479, val_loss: 0.0521\n",
      "Epoch [451/500], Loss: 0.0326, val_loss: 0.0408\n",
      "Epoch [461/500], Loss: 0.0365, val_loss: 0.0703\n",
      "Epoch [471/500], Loss: 0.0319, val_loss: 0.0446\n",
      "Epoch [481/500], Loss: 0.0290, val_loss: 0.0378\n",
      "Epoch [491/500], Loss: 0.0279, val_loss: 0.0433\n",
      "0.2598596540634523\n",
      "training start ...\n",
      "Epoch [1/500], Loss: 0.6921, val_loss: 0.2846\n",
      "Epoch [11/500], Loss: 0.1230, val_loss: 0.1051\n",
      "Epoch [21/500], Loss: 0.1311, val_loss: 0.2047\n",
      "Epoch [31/500], Loss: 0.1349, val_loss: 0.1140\n",
      "Epoch [41/500], Loss: 0.1111, val_loss: 0.0987\n",
      "Epoch [51/500], Loss: 0.0986, val_loss: 0.1031\n",
      "Epoch [61/500], Loss: 0.1224, val_loss: 0.1278\n",
      "Epoch [71/500], Loss: 0.0968, val_loss: 0.0931\n",
      "Epoch [81/500], Loss: 0.1124, val_loss: 0.0932\n",
      "Epoch [91/500], Loss: 0.0885, val_loss: 0.0751\n",
      "Epoch [101/500], Loss: 0.0948, val_loss: 0.0818\n",
      "Epoch [111/500], Loss: 0.0883, val_loss: 0.0826\n",
      "Epoch [121/500], Loss: 0.0878, val_loss: 0.0690\n",
      "Epoch [131/500], Loss: 0.0939, val_loss: 0.0798\n",
      "Epoch [141/500], Loss: 0.0804, val_loss: 0.0620\n",
      "Epoch [151/500], Loss: 0.0745, val_loss: 0.0604\n",
      "Epoch [161/500], Loss: 0.0858, val_loss: 0.0855\n",
      "Epoch [171/500], Loss: 0.0708, val_loss: 0.0666\n",
      "Epoch [181/500], Loss: 0.0753, val_loss: 0.1140\n",
      "Epoch [191/500], Loss: 0.0656, val_loss: 0.0872\n",
      "Epoch [201/500], Loss: 0.0771, val_loss: 0.0560\n",
      "Epoch [211/500], Loss: 0.0669, val_loss: 0.0983\n",
      "Epoch [221/500], Loss: 0.0706, val_loss: 0.0548\n",
      "Epoch [231/500], Loss: 0.0611, val_loss: 0.0517\n",
      "Epoch [241/500], Loss: 0.0649, val_loss: 0.0503\n",
      "Epoch [251/500], Loss: 0.0756, val_loss: 0.0494\n",
      "Epoch [261/500], Loss: 0.0648, val_loss: 0.0464\n",
      "Epoch [271/500], Loss: 0.0618, val_loss: 0.0508\n",
      "Epoch [281/500], Loss: 0.0556, val_loss: 0.0516\n",
      "Epoch [291/500], Loss: 0.0548, val_loss: 0.0815\n",
      "Epoch [301/500], Loss: 0.0518, val_loss: 0.0696\n",
      "Epoch [311/500], Loss: 0.0506, val_loss: 0.0517\n",
      "Epoch [321/500], Loss: 0.0492, val_loss: 0.0480\n",
      "Epoch [331/500], Loss: 0.0528, val_loss: 0.0450\n",
      "Epoch [341/500], Loss: 0.0415, val_loss: 0.0348\n",
      "Epoch [351/500], Loss: 0.0410, val_loss: 0.0417\n",
      "Epoch [361/500], Loss: 0.0410, val_loss: 0.0361\n",
      "Epoch [371/500], Loss: 0.0398, val_loss: 0.0446\n",
      "Epoch [381/500], Loss: 0.0420, val_loss: 0.0382\n",
      "Epoch [391/500], Loss: 0.0407, val_loss: 0.0681\n",
      "Epoch [401/500], Loss: 0.0417, val_loss: 0.0355\n",
      "Epoch [411/500], Loss: 0.0355, val_loss: 0.0411\n",
      "Epoch [421/500], Loss: 0.0327, val_loss: 0.0290\n",
      "Epoch [431/500], Loss: 0.0339, val_loss: 0.0289\n",
      "Epoch [441/500], Loss: 0.0365, val_loss: 0.0310\n",
      "Epoch [451/500], Loss: 0.0280, val_loss: 0.0342\n",
      "Epoch [461/500], Loss: 0.0279, val_loss: 0.0370\n",
      "Epoch [471/500], Loss: 0.0259, val_loss: 0.0289\n",
      "Epoch [481/500], Loss: 0.0230, val_loss: 0.0240\n",
      "Epoch [491/500], Loss: 0.0291, val_loss: 0.0306\n",
      "0.186296103907691\n",
      "training start ...\n",
      "Epoch [1/500], Loss: 0.6698, val_loss: 0.2596\n",
      "Epoch [11/500], Loss: 0.1401, val_loss: 0.1115\n",
      "Epoch [21/500], Loss: 0.1548, val_loss: 0.1843\n",
      "Epoch [31/500], Loss: 0.1057, val_loss: 0.1212\n",
      "Epoch [41/500], Loss: 0.1056, val_loss: 0.1070\n",
      "Epoch [51/500], Loss: 0.1102, val_loss: 0.1074\n",
      "Epoch [61/500], Loss: 0.0950, val_loss: 0.0944\n",
      "Epoch [71/500], Loss: 0.0901, val_loss: 0.1263\n",
      "Epoch [81/500], Loss: 0.0871, val_loss: 0.0993\n",
      "Epoch [91/500], Loss: 0.1010, val_loss: 0.0972\n",
      "Epoch [101/500], Loss: 0.0960, val_loss: 0.1232\n",
      "Epoch [111/500], Loss: 0.1131, val_loss: 0.1091\n",
      "Epoch [121/500], Loss: 0.1096, val_loss: 0.0983\n",
      "Epoch [131/500], Loss: 0.0784, val_loss: 0.0980\n",
      "Epoch [141/500], Loss: 0.0767, val_loss: 0.1092\n",
      "Epoch [151/500], Loss: 0.0694, val_loss: 0.0891\n",
      "Epoch [161/500], Loss: 0.0743, val_loss: 0.0883\n",
      "Epoch [171/500], Loss: 0.0725, val_loss: 0.1142\n",
      "Epoch [181/500], Loss: 0.0800, val_loss: 0.0807\n",
      "Epoch [191/500], Loss: 0.0751, val_loss: 0.1101\n",
      "Epoch [201/500], Loss: 0.0745, val_loss: 0.0824\n",
      "Epoch [211/500], Loss: 0.0723, val_loss: 0.0918\n",
      "Epoch [221/500], Loss: 0.0715, val_loss: 0.0980\n",
      "Epoch [231/500], Loss: 0.0707, val_loss: 0.0727\n",
      "Epoch [241/500], Loss: 0.0602, val_loss: 0.0912\n",
      "Epoch [251/500], Loss: 0.0611, val_loss: 0.0779\n",
      "Epoch [261/500], Loss: 0.0549, val_loss: 0.0694\n",
      "Epoch [271/500], Loss: 0.0583, val_loss: 0.0814\n",
      "Epoch [281/500], Loss: 0.0519, val_loss: 0.0696\n",
      "Epoch [291/500], Loss: 0.0497, val_loss: 0.0680\n",
      "Epoch [301/500], Loss: 0.0487, val_loss: 0.0544\n",
      "Epoch [311/500], Loss: 0.0466, val_loss: 0.0582\n",
      "Epoch [321/500], Loss: 0.0643, val_loss: 0.0767\n",
      "Epoch [331/500], Loss: 0.0448, val_loss: 0.0482\n",
      "Epoch [341/500], Loss: 0.0477, val_loss: 0.0726\n",
      "Epoch [351/500], Loss: 0.0412, val_loss: 0.0472\n",
      "Epoch [361/500], Loss: 0.0402, val_loss: 0.0515\n",
      "Epoch [371/500], Loss: 0.0405, val_loss: 0.0464\n",
      "Epoch [381/500], Loss: 0.0362, val_loss: 0.0418\n",
      "Epoch [391/500], Loss: 0.0364, val_loss: 0.0420\n",
      "Epoch [401/500], Loss: 0.0338, val_loss: 0.0512\n",
      "Epoch [411/500], Loss: 0.0324, val_loss: 0.0426\n",
      "Epoch [421/500], Loss: 0.0316, val_loss: 0.0402\n",
      "Epoch [431/500], Loss: 0.0342, val_loss: 0.0386\n",
      "Epoch [441/500], Loss: 0.0265, val_loss: 0.0475\n",
      "Epoch [451/500], Loss: 0.0280, val_loss: 0.0451\n",
      "Epoch [461/500], Loss: 0.0270, val_loss: 0.0437\n",
      "Epoch [471/500], Loss: 0.0322, val_loss: 0.0345\n",
      "Epoch [481/500], Loss: 0.0231, val_loss: 0.0292\n",
      "Epoch [491/500], Loss: 0.0232, val_loss: 0.0277\n",
      "0.26252962625651727\n",
      "training start ...\n",
      "Epoch [1/500], Loss: 0.5436, val_loss: 0.2688\n",
      "Epoch [11/500], Loss: 0.1479, val_loss: 0.1374\n",
      "Epoch [21/500], Loss: 0.1165, val_loss: 0.1285\n",
      "Epoch [31/500], Loss: 0.1212, val_loss: 0.1029\n",
      "Epoch [41/500], Loss: 0.1051, val_loss: 0.1165\n",
      "Epoch [51/500], Loss: 0.1033, val_loss: 0.0944\n",
      "Epoch [61/500], Loss: 0.1384, val_loss: 0.1014\n",
      "Epoch [71/500], Loss: 0.0896, val_loss: 0.1012\n",
      "Epoch [81/500], Loss: 0.0839, val_loss: 0.0807\n",
      "Epoch [91/500], Loss: 0.1043, val_loss: 0.1458\n",
      "Epoch [101/500], Loss: 0.0849, val_loss: 0.0731\n",
      "Epoch [111/500], Loss: 0.0828, val_loss: 0.0845\n",
      "Epoch [121/500], Loss: 0.0854, val_loss: 0.0662\n",
      "Epoch [131/500], Loss: 0.0749, val_loss: 0.0725\n",
      "Epoch [141/500], Loss: 0.0754, val_loss: 0.0651\n",
      "Epoch [151/500], Loss: 0.0742, val_loss: 0.1105\n",
      "Epoch [161/500], Loss: 0.0928, val_loss: 0.0682\n",
      "Epoch [171/500], Loss: 0.0742, val_loss: 0.0905\n",
      "Epoch [181/500], Loss: 0.0751, val_loss: 0.0775\n",
      "Epoch [191/500], Loss: 0.0748, val_loss: 0.0643\n",
      "Epoch [201/500], Loss: 0.0952, val_loss: 0.0717\n",
      "Epoch [211/500], Loss: 0.0730, val_loss: 0.0570\n",
      "Epoch [221/500], Loss: 0.0622, val_loss: 0.0610\n",
      "Epoch [231/500], Loss: 0.0594, val_loss: 0.0751\n",
      "Epoch [241/500], Loss: 0.0597, val_loss: 0.0582\n",
      "Epoch [251/500], Loss: 0.0691, val_loss: 0.0571\n",
      "Epoch [261/500], Loss: 0.0500, val_loss: 0.0481\n",
      "Epoch [271/500], Loss: 0.0607, val_loss: 0.0669\n",
      "Epoch [281/500], Loss: 0.0586, val_loss: 0.0421\n",
      "Epoch [291/500], Loss: 0.0576, val_loss: 0.0453\n",
      "Epoch [301/500], Loss: 0.0551, val_loss: 0.0443\n",
      "Epoch [311/500], Loss: 0.0532, val_loss: 0.0513\n",
      "Epoch [321/500], Loss: 0.0428, val_loss: 0.0426\n",
      "Epoch [331/500], Loss: 0.0462, val_loss: 0.0377\n",
      "Epoch [341/500], Loss: 0.0652, val_loss: 0.0565\n",
      "Epoch [351/500], Loss: 0.0527, val_loss: 0.0332\n",
      "Epoch [361/500], Loss: 0.0400, val_loss: 0.0376\n",
      "Epoch [371/500], Loss: 0.0351, val_loss: 0.0318\n",
      "Epoch [381/500], Loss: 0.0458, val_loss: 0.0595\n",
      "Epoch [391/500], Loss: 0.0365, val_loss: 0.0419\n",
      "Epoch [401/500], Loss: 0.0444, val_loss: 0.0342\n",
      "Epoch [411/500], Loss: 0.0311, val_loss: 0.0318\n",
      "Epoch [421/500], Loss: 0.0294, val_loss: 0.0326\n",
      "Epoch [431/500], Loss: 0.0274, val_loss: 0.0386\n",
      "Epoch [441/500], Loss: 0.0278, val_loss: 0.0277\n",
      "Epoch [451/500], Loss: 0.0247, val_loss: 0.0311\n",
      "Epoch [461/500], Loss: 0.0297, val_loss: 0.0385\n",
      "Epoch [471/500], Loss: 0.0314, val_loss: 0.0485\n",
      "Epoch [481/500], Loss: 0.0236, val_loss: 0.0247\n",
      "Epoch [491/500], Loss: 0.0260, val_loss: 0.0369\n",
      "0.19127652578841478\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "Ypred = np.zeros_like(Y)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    X_tr = X[train_index]\n",
    "    X_val = X[val_index]\n",
    "    y_tr = Y[train_index]\n",
    "    y_val = Y[val_index]\n",
    "\n",
    "    train_loader = get_loader(X_tr, np.log(y_tr))\n",
    "    valid_loader = get_loader(X_val, np.log(y_val))\n",
    "    \n",
    "    model = train_model(train_loader, valid_loader)\n",
    "    models.append(model)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = np.exp(model(torch.tensor(X_val).float().to(device)))\n",
    "        Ypred[val_index] = y_pred.flatten()\n",
    "    \n",
    "    print(error(y_val, y_pred))\n",
    "    \n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22187952063239444"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error(Y, Ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions['MLP'] = Ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの相関"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr = submissions.corr()\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test[features].fillna(0.).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "\n",
    "for m in models:\n",
    "    test_pred = np.exp(m.predict(X_test))\n",
    "    test_preds.append(test_pred)\n",
    "\n",
    "test_preds = np.vstack(test_preds).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test[['Id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['SalePrice'] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../output/base_line.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
